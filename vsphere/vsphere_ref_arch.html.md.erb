---
title: vSphere Reference Architecture
owner: Customer0
---

This guide presents a reference architecture for Pivotal Cloud Foundry (PCF) on vSphere. It builds on the common base architectures described in [Platform Architecture and Planning](../index.html). 

See [Installing PCF on vSphere](../../customizing/vsphere.html) for additional requirements and installation instructions for running PCF on vSphere.

## <a id="overview"></a> Overview

The vSphere reference architecture for PAS and PKS is based on software-defined networking (SDN) infrastructure.

VMware vSphere includes NSX-T and NSX-V data centers that support SDN infrastructure. 

NSX-T Data Center is a software-defined network (SDN) virtualization platform that runs on VMware ESXi virtual hosts.

NSX-V Data Center is

Using the available NSX-T Container Plug-in (NCP) tile enables a sophisticated container networking stack in place of the built-in “silk” stack and interlocks with NSX-T already deployed in the IaaS. The NCP tile can not be used unless NSX-T has already been established in the IaaS. These technologies can not be retrofitted into an established PKS or PAS installation

Expanding PAS with SDN features is best considered as a greenfield effort. Inserting an SDN layer under a working PAS installation is non-trivial and likely will trigger a rebuild. NSX-T constructs that can be used by PAS include:

* Logical Networks, encapsulated broadcast domains
* VLAN exhaustion avoidance thru the use of Logical Networks
* Routing services and NAT/SNAT to network fence the PCF installation
* Load balancing services to pool systems such as gorouters
* SSL termination at the load balancer
* Distributed routing and firewalling services at the hypervisor


## <a id="pas-nsx-t"></a> PAS on vSphere with NSX-T

The reference architecture in this section includes VMware vSphere and NSX-T Data Center.

In NSX-T architecure, PAS is deployed on a 3 cluster/AZ defined front ended by a NSX-T Tier-0 router which acts as a central logical router into the PAS platform. Tier-1 routers created for each deployment such as PAS subnet, services subnet connects back to Tier-0 routers. Static or dynamic routing (BGP) from the IP backbone into NSX-T can be configured to Tier-0 router. 

To deploy PCF without NSX-T, see [PAS on vSphere with NSX-V](), which includes NSX-V design considerations.

### <a id="pas-nsx-t-diagram"></a> Architecture

The following diagram shows PAS on vSphere with NSX-T. 

### <a id="pas-nsx-t-networking"></a> Networking

The following section describe networking considerations for PAS on vSphere with NSX-T.

#### <a id="pas-nsx-t-dns"></a> DNS

Wildcard domain names for PAS must be configured in DNS and must be registered. 

For more information, see the [Domain Names](./index.html#pas-domains) section in _Platform Planning and Architecture_.

#### <a id="pas-nsx-t-load-balancing"></a> Load Balancing

Pivotal recommends that you use standard NSX-T load balancers for a PAS on vSphere with NSX-T installation.  

Layer 4 and Layer 7 NSX-T load balancers are created automatically upon app deployment. Pivotal recommends that you terminate SSL at the Gorouters and use the NSX-T load balancer as a pass-through.

You must configure NSX-T load balancers for the Gorouters. The PAS system and apps domains should resolve to this load balancer and have either a private or a public IP address assigned to them. 

Any TCP routers and SSH Proxies also require load balancers.

#### <a id="pas-nsx-t-networking-subnets-ip"></a> Networking, Subnets, and IP Spacing Planning

Networks defined for PAS is outlined in base ref arch section https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pas-network.

PAS requires statically-defined networks to host PAS component VMs. 

The tenant side of an NSX-T deployment requires a series of non-routable address banks. NSX-T routers manage the address banks.

Dynamically assigned org networks are attached to automatically-generated NSX-T Tier-1 routers. The operator does not define these networks in Ops Manager, but allows for them to get built by supplying a non-overlapping block of address space for this purpose. This is configurable in NCP Configuration in the NSX-T tile in Ops Manager. The default is /24 (every Org gets a new /24).

This reference uses a pattern that follows previous references, with the main difference being all networks break on the /24 boundary (for easier CIDR math) and the network octet is now numerically sequential (1-2-3).

#### <a id="pas-nsx-t-networks"></a> Networks

Routable external IPs on the Provider side, for example for NATs, PAS Orgs and load balancers get assigned to the T0 router front-ending the PCF installation. There are two approaches to assigning address space blocks to this job, each with its own ramifications.

The T0 router needs some routable address space to advertise on the BGP network with its peers. Select a network range with ample address space that can be split into two logical jobs, one job being advertised as a route for traffic and the other job is for aligning T0 DNATs/SNATs, load balancers and other jobs. Unlike NSX-V, you will consume much more address space for SNATs than before.


### <a id="pas-nsx-t-ha"></a> High Availability

(Link to Base Ref arch High availability section)
https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pas-ha


### <a id="pas-nsx-t-storage"></a> Storage

(Link to Base Ref arch Storage section)
https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pas-storage

Shared storage is a requirement for PCF. You can allocate networked storage to the host clusters following one of two common approaches, _horizontal_ or _vertical_. The approach you follow should reflect how your data center arranges its storage and host blocks in its physical layout.

Horizontal: You grant all hosts access to all datastores and assign a subset to each installation.

For example, with six datastores `ds01` through `ds06`, you grant all nine hosts access to all six datastores. You then provision your first PCF installation to use stores `ds01` through `ds03`, and your second PCF installation to use `ds04` through `ds06`.

Vertical: You grant each cluster its own dedicated datastores, creating a "cluster-aligned" storage strategy. vSphere VSAN is an example of this architecture. 

For example, with six datastores `ds01` through `ds06`, you assign datastores `ds01` and `ds02` to your first cluster, `ds03` and `ds04` to your second cluster, and `ds05` and `ds06` to your third cluster. You then provision your first PCF installation to use `ds01`, `ds03`, and `ds05`, and your second PCF installation to use `ds02`, `ds04`, and `ds06`. With this arrangement, all VMs in the same installation and cluster share a dedicated datastore.

Note: If a datastore is part of a vSphere Storage Cluster using sDRS (storage DRS), you must disable the s-vMotion feature on any datastores used by PCF. Otherwise, s-vMotion activity can rename independent disks and cause BOSH to malfunction. For more information, see <a href="./vsphere_migrate_datastore.html">How to Migrate PCF to a New Datastore in vSphere</a>.
Storage Capacity and Type

Storage Capacity

Pivotal recommends the following capacity allocation for PAS installations:

- For production use, at least 8 TB of data storage, either as one 8 TB store or a number of smaller volumes adding up to 8 TB. Frequently-used development may require significantly more storage to accommodate new code and buildpacks. 

- For small installations without many tiles, 4-6 TB.

The primary consumer of storage is the NFS/WebDAV blobstore. 

<p class="note"><strong>Note</strong>: PCF does not currently support using vSphere Storage Clusters with the <a href="#overview">latest versions of PCF</a> validated for the reference architecture. Datastores should be listed in the vSphere tile by their native name, not the cluster name created by vCenter for the storage cluster.</p>


### <a id="pas-nsx-t-sql"></a> SQL Server


### <a id="pas-nsx-t-securty"></a> Security


### <a id="pas-nsx-t-blobstore"></a> Blobstore Storage Account


### <a id="pas-nsx-t-pas"></a> PAS Components

NCP Tile for PAS needs to be configured and deployed at the same time when deploying PAS runtime.  Flannel or NSX-T can be considered for network container plugin for PAS on NSX-T

## <a id="pas-nsx-v"></a> PAS on vSphere with NSX-V

### <a id="pas-nsx-t-diagram"></a> Architecture

The following diagram shows PAS on vSphere with NSX-V. 

### <a id="pas-nsx-t-networking"></a> Networking

#### DNS

#### 

### <a id="pas-nsx-t-ha"></a> High Availability

### <a id="pas-nsx-t-storage"></a> Storage

### <a id="pas-nsx-t-sql"></a> SQL Server

### <a id="pas-nsx-t-security"></a> Security

### <a id="pas-nsx-t-blobstore"></a> Blobstore Storage Accounts

## <a id="pks-nsx-t"></a> PKS on vSphere with NSX-T

Pivotal Container Service (PKS) is supplied with NSX-T SDN included, along with the associated licensing. The assumption is that the reader will want to use all of this technology at once to get full benefit from both. 
NSX-T SDN bundles built-in Network container plugin(NCP), however one could also use Flannel network stack for container networking.

New T1 routers will be spawned on-demand as new clusters and namespaces are added to PKS. These can grow rapidly, so a large address block is desired for this use. Allocate a large POD IP block like /14 in NSX-T and NSX-T carves out address blocks of /24 by default. This CIDR range for kubernetes services network ranges  is configurables from operations manager. Reference POD IP block from PKS tile configuration. 


PKS 1.2.x and above, multiple master nodes is a supported feature. The number of master nodes is defined per plan in the PKS tile in Ops Manager and should be an odd number to allow etcd to form a quorum. It is recommended to have at least 1 master node per AZ for HA and DR.

With the dynamic creation of networks for both PKS clusters and pods, multi-tenancy becomes a discussion topic. It is recommended to use multiple clusters as opposed to a single cluster with multiple namespaces. Multiple clusters provide security, customization-per-cluster, privileged containers, failure domains, version choice, etc. Namespaces are a naming construct as opposed to a tenancy construct.


### <a id="pas-nsx-t-diagram"></a> Architecture

The following diagram shows PKS on vSphere with NSX-T. 

### <a id="pas-nsx-t-networking"></a> Networking

#### Load Balancing

Standard  NSX-T Load Balancer  are preferred Load Balancers on PKS  on vSphere NSX-T installation.  NSX-T Load Balancers of type Layer 4 and Layer 7 are created automatically upon application deployment.. SSL can  be terminated at the load balancer.

How you select ingress routing influences load balancing choices. You’re going to want both.

Ingress Routing - Layer 7
Service Type:LoadBalancer - Layer 4
Ingress Routing is provided natively by NSX-T.  Third party options include Istio or Nginx running as containers in the cluster. Wildcard DNS entries are needed to point at the ingress service in the style of go-routers in PAS. Domain info for ingress is defined in the manifest of the kubernetes deployment. Here is an example.

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: music-ingress
  namespace: music1
spec:
  rules:
  - host: music1.pks.domain.com
    http:
      paths:
      - path: /.*
        backend:
          serviceName: music-service
          servicePort: 8080
```

Service Type:LoadBalancer - Layer 4
When pushing a kubernetes deployment with type set to “LoadBalancer”, NSX-T automatically creates a new VIP for the deployment on the existing load balancer for that namespace. You will need to specify a listening and translation port in the service, along with a name for tagging. You will also specify a protocol to use. Here is an example.

apiVersion: v1
kind: Service
metadata:
  ...
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: web



#### Networks, Subnets, and IP Spacing Planning

Networks defined for PKS is outlined in base ref arch section https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pks-network.
In addition , allocate large block following network CIDRs. These can be configured during PKS cluster configuration in OpsManager.
PKS Clusters 172.24.0.0/14
PKS Pods 172.28.0.0/14

When deploying PKS via Ops Manager, you must allow for a block of address space for dynamic networks that PKS will deploy per namespace. The recommended address space is distinctly different  that used with PAS, to give the operator a quick visual queue as to which jobs relate to what service.

When new PKS cluster is  created, PKS dynamically creates a new /24 network is carved out from PKS cluster address space.  When a new application is deployed, a new NSX-T Tier-1 routers are generated and /24 is created from the PKS PODS network.

So, to summarize, do allocate a large address block for PKS clusters and PODS block.

#### Networks

Network Requirements for External Routing:

Routable external IPs on the Provider side, for example for NATs, PAS Orgs and load balancers get assigned to the T0 router front-ending the PCF installation. There are two approaches to assigning address space blocks to this job, each with its own ramifications.

Without PKS at all, or With PKS Ingress: The T0 router needs some routable address space to advertise on the BGP network with its peers. Select a network range with ample address space that can be split into two logical jobs, one job being advertised as a route for traffic and the other job is for aligning T0 DNATs/SNATs, load balancers and other jobs. Unlike NSX-V, you will consume much more address space for SNATs than before.

With PKS No Ingress: Relative to above, this approach has much higher address space consumption for load balancer VIPs, so allow for 4x the address space due to the fact that Kubernetes service types allocate addresses very frequently.

Reference: Kubernetes Ingress

Provider Routable Address Space: /25 (or /23 for PKS No Ingress)

NSX-T handles the routing between a T0 router and any T1 routers associated with it. There are no design parameters needed from Pivotal to accomplish this. For the purposes of PCF, the T0/T1 routing relationship “just works”.


### <a id="pas-nsx-t-ha"></a> High Availability

(Link to Base Ref arch High availability section)
https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pks-ha


### <a id="pas-nsx-t-storage"></a> Storage

Pivotal Container service(PKS) on vSphere supports stateful apps using Persistent Volumes (PVs). PKS on vSphere supports static persistent volume provisioning and dynamic persistent volume provisioning.  These storage options to support stateful apps making PVs claims include vSAN datastores and Network File Share(NFS) or VMFS over iSCI or Fiber channel datastores. 
vSAN datastores typically has data boundary limits to single vSphere cluster, thus limiting static/dynamic PVs to single Availability Zone(AZ). When configured as multi cluster , multi AZs deployment, there will be a need to provide a shared storage such as VMFS/NFS over iSCI/FC.  Storage considerations are well documented in this https://docs.pivotal.io/runtimes/pks/1-4/vsphere-persistent-storage.html


### <a id="pas-nsx-t-security"></a> Security

https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pks-security

### <a id="pas-nsx-t-blobstore"></a> Blobstore Storage Accounts

## <a id="pas-pks"></a> PAS and PKS on vSphere with NSX-T

A fully meshed installation of PCF v2.2 or later designed using the recommendations provided in this topic looks as follows:

<%= image_tag('../images/pas-pks-nsxt.png') %>

[View a larger version of this diagram](../images/pas-pks-nsxt.png)

Common components are the NSX T0 router and the associated T1 routers.
This approach ensures that any cross-traffic between PKS and PAS apps stays within the bounds of the T0.
This also provides a one-stop access point to the whole installation, which simplifies deployment automation for multiple, identical installations.

AZs are aligned to vSphere clusters, with resource pools as an optional organizational tool to place multiple foundations into the same capacity.
You can align PKS to any AZ or cluster.
Keeping PKS and PAS in completely separate AZs is not required.

You can use resource pools in vSphere clusters as AZ constructs to stack different installations of PCF. 
As server capacity continues to increase, the efficiency of deploying independent server clusters only for one installation is low. For example, customers are commonly deploying servers with 768-GB RAM and greater.

To allow for max capacity growth, consider using an NSX-T installation per foundation.

If you want to split the PAS and PKS installations into separate network trees, behind separate T0 routers, ensure that approach meets your needs by reviewing VMware’s recommendations for T0 to T0 routing.
For more information,
see [Reference Design Guide for PAS and PKS with VMware NSX-T Data Center](https://communities.vmware.com/docs/DOC-38609) in the VMware documentation.
