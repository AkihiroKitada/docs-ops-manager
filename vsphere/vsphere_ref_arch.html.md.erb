---
title: vSphere Reference Architecture
owner: Customer0
---

This topic provides reference architectures for Pivotal Cloud Foundry (PCF) on vSphere. It builds on the common base architectures described in [Platform Architecture and Planning](../index.html). 

See [Installing PCF on vSphere](../../customizing/vsphere.html) for additional requirements and installation instructions for PCF on vSphere.

## <a id="overview"></a> Overview

The vSphere reference architecture for the Pivotal Application Service (PAS) and Pivotal Container Service (PKS) runtimes is based on software-defined networking (SDN) infrastructure. vSphere offers NSX-T and NSX-V to support SDN infrastructure.

PCF supports the following configurations for PCF on vSphere deployments:

* PAS on vSphere with NSX-T. See [PAS on vSphere with NSX-T](#pas-nsx-t).
* PAS on vSphere with NSX-V. See [PAS on vSphere with NSX-V](#pas-nsx-v).
* PKS on vSphere with NSX-T. See [PKS on vSphere with NSX-T](#pks-nsx-t).


## <a id="pas-nsx-t"></a> PAS on vSphere with NSX-T

The reference architecture in this section describes the architecture for PAS on vSphere with NSX-T deployments.

This section also includes considerations and requirments for PAS on vSphere with NSX-T deployments, such as considerations related to storage, networking, and high availability.


### <a id="pas-nsx-t-diagram"></a> Architecture

The following diagram shows PAS on vSphere with NSX-T.


INSERT DIAGRAM HERE


In NSX-T architecure, PAS is deployed with three clusters and three availability zones (AZs). A NSX-T Tier-0 router is on the front end of the PAS deployment. This router is a central logical router into the PAS platform. 

Tier-1 routers, such as the router for the PAS subnet, connect to the Tier-0 router. Static or dynamic BGP routing from the IP backbone into NSX-T can be configured to the Tier-0 router. 


### <a id="pas-nsx-t-constructs"></a> NSX-T Constructs

The NSX-T Container Plug-in (NCP) tile enables a sophisticated container networking stack and interlocks with NSX-T. The NCP tile can not be used unless NSX-T has already been established in the IaaS. 

These technologies can not be retrofitted into an established PKS or PAS installation.

NSX-T constructs that can be used by PAS include:

* Logical Networks, encapsulated broadcast domains
* VLAN exhaustion avoidance with the use of Logical Networks
* Routing services and NAT/SNAT to network fence the PCF installation
* Load balancing services to pool systems such as gorouters
* SSL termination at the load balancer
* Distributed routing and firewalling services at the hypervisor


### <a id="pas-nsx-t-networking"></a> Networking

The following sections describe networking considerations for PAS on vSphere with NSX-T deployments.

#### <a id="pas-nsx-t-networks"></a> Networks for NSX-T T0 Router 

Routable external IPs on the Provider side, such as external IPs for NATs or load balancers, are assigned to the T0 router on the front end of the PCF deployment. 

The T0 router must have routable address space to advertise on the BGP network with its peers. 

Select a network range with ample address space that can be split into two logical jobs. One job is a route for traffic. The other job aligns T0 DNATs/SNATs, load balancers, and other jobs. 

<p class="note"><b>Note: </b> This configuration consumes much more address space for SNATs as compared to NSX-V.</p>

#### <a id="pas-nsx-t-dns"></a> DNS

PAS requires a system domain, app domain, and several wildcard domains. 

For more information about DNS requirements for PAS, see the [Domain Names](./index.html#pas-domains) section in _Platform Planning and Architecture_.

#### <a id="pas-nsx-t-load-balancing"></a> Load Balancing

The following are load balancing considerations for PAS on vSphere with NSX-T deployments:

* Pivotal recommends that you use standard NSX-T load balancers.  
* Layer 4 and Layer 7 NSX-T load balancers are created automatically during app deployment. Pivotal recommends that you terminate SSL at the Gorouters and use the NSX-T load balancers as a pass-through.
* You must configure NSX-T load balancers for the Gorouters. The domains for both the PAS system and the PAS apps must resolve to this load balancer and have either a private or a public IP address assigned to them. 
* Any TCP routers and SSH Proxies also require NSX-T load balancers.

#### <a id="pas-nsx-t-networking-subnets-ip"></a> Networking, Subnets, and IP Spacing

The following are considerations related to networks, subnets, and IP spacing for PAS on vSphere with NSX-T deployments:

* PAS requires statically-defined networks to host PAS component VMs. 
* The tenant side of a NSX-T deployment requires a series of non-routable address banks. NSX-T routers manage the address banks.

<p class="note"><b>Note:</b> This reference architecture uses a pattern in which all networks break on the `/24` boundary and the network octet is numerically sequential.</p>

For more information about PAS subnets, see the [Required Subnets](./index.html#pas-network) section of the _Platform Architecure and Planning_ topic.

#### <a id="pas-nsx-t-networking-dynamic-org-networks"></a> PAS Org Networks

Dynamically assigned PAS org networks are attached to NSX-T Tier-1 routers. These org networks are automatically constructed based on a non-overlapping block of address space. 

You can configure the block of address space in the **NCP Configuration** section of the NSX-T tile in Ops Manager. The default is `/24`, which means that every org in PAS is assigned a new `/24`.


### <a id="pas-nsx-t-ha"></a> High Availability

For information about high availability considerations for PAS on vSphere, see the [High Availability](./index.html#pas-ha) section of _Platform Architecture and Planning_.


### <a id="pas-nsx-t-storage"></a> Storage

The following are storage capacity considerations for PAS on vSphere with NSX-T deployments.

<p class="note"><b>Note:</b> If a datastore is part of a vSphere Storage Cluster using DRS storage (sDRS), you must disable the s-vMotion feature on any datastores used by PCF. Otherwise, s-vMotion activity can rename independent disks and cause BOSH to malfunction. For more information, see <a href="./vsphere_migrate_datastore.html">How to Migrate PCF to a New Datastore in vSphere</a>.</p>

Pivotal recommends the following storage capacity allocation for PAS installations:

* For production environments: At least 8 TB of data storage, either as one 8 TB store or a number of smaller volumes that sum to 8 TB. Frequently-used developments may require significantly more storage to accommodate new code and buildpacks. 
* For small PAS installations: 4 to 6 TB of data storage.

<p class="note"><b>Note:</b> The primary consumer of storage is the NFS/WebDAV blobstore.</p> 

For more information about general storage considerations for PAS, see the
[Storage](./index.html#pas-storage) section of _Platform Architecture and Planning_.

<p class="note"><strong>Note</strong>: PCF does not support using vSphere Storage Clusters with the <a href="#overview">latest versions of PCF</a> validated for the reference architecture. Datastores should be listed in the vSphere tile by their native name, not the cluster name created by vCenter for the storage cluster.</p>


### <a id="pas-nsx-t-sql"></a> SQL Server

An internal MySQL database is sufficient for use in production environments. 

However, an external database provides more control over database management for large environments that require multiple data centers. 

For information about configuring system databases on PAS, see the [Configure System Databases](../customizing/configure-pas.html#sys-db) section of _Configuring PAS_.


### <a id="pas-nsx-t-securty"></a> Security

For information about security considerations for PAS deployments, see the [Security](./index.html#pas-security) section of _Platform Architecture and Planning_.


### <a id="pas-nsx-t-blobstore"></a> Blobstore Storage

Pivotal recommends that you use the following blobstore storage for production and non-production PAS environments:

* **Non-production environments**: Use a NFS/WebDAV blobstore.
* **Production environments**: Use a S3 storage appliance as the blobstore.

For non-production environments, the NFS/WebDAV blobstore can be the primary consumer of storage, as the NFS/WebDAV blobstore must be actively maintained. There will be down time for deployment during events such as storage upgrades or migrations to new disks.   

For more information about blobstore storage considerations, see the [Configure File Storage](../upgrading/configuring.html#file-storage) section of _Configuring PAS for Upgrades_.

### <a id="pas-nsx-t-pas"></a> PAS Components

NCP Tile for PAS needs to be configured and deployed at the same time when deploying PAS runtime.  Flannel or NSX-T can be considered for network container plugin for PAS on NSX-T

## <a id="pas-nsx-v"></a> PAS on vSphere with NSX-V

### <a id="pas-nsx-t-diagram"></a> Architecture

The following diagram shows PAS on vSphere with NSX-V. 

### <a id="pas-nsx-t-networking"></a> Networking

#### DNS

#### 

### <a id="pas-nsx-t-ha"></a> High Availability

### <a id="pas-nsx-t-storage"></a> Storage

### <a id="pas-nsx-t-sql"></a> SQL Server

### <a id="pas-nsx-t-security"></a> Security

### <a id="pas-nsx-t-blobstore"></a> Blobstore Storage Accounts

## <a id="pks-nsx-t"></a> PKS on vSphere with NSX-T

Pivotal Container Service (PKS) is supplied with NSX-T SDN included, along with the associated licensing. The assumption is that the reader will want to use all of this technology at once to get full benefit from both. 
NSX-T SDN bundles built-in Network container plugin(NCP), however one could also use Flannel network stack for container networking.

New T1 routers will be spawned on-demand as new clusters and namespaces are added to PKS. These can grow rapidly, so a large address block is desired for this use. Allocate a large POD IP block like /14 in NSX-T and NSX-T carves out address blocks of /24 by default. This CIDR range for kubernetes services network ranges  is configurables from operations manager. Reference POD IP block from PKS tile configuration. 


PKS 1.2.x and above, multiple master nodes is a supported feature. The number of master nodes is defined per plan in the PKS tile in Ops Manager and should be an odd number to allow etcd to form a quorum. It is recommended to have at least 1 master node per AZ for HA and DR.

With the dynamic creation of networks for both PKS clusters and pods, multi-tenancy becomes a discussion topic. It is recommended to use multiple clusters as opposed to a single cluster with multiple namespaces. Multiple clusters provide security, customization-per-cluster, privileged containers, failure domains, version choice, etc. Namespaces are a naming construct as opposed to a tenancy construct.


### <a id="pks-nsx-t-diagram"></a> Architecture

The following diagram shows PKS on vSphere with NSX-T. 

### <a id="pks-nsx-t-networking"></a> Networking

#### <a id="pks-nsx-t-load-balancing"></a> Load Balancing

Standard  NSX-T Load Balancer  are preferred Load Balancers on PKS  on vSphere NSX-T installation.  NSX-T Load Balancers of type Layer 4 and Layer 7 are created automatically upon application deployment.. SSL can  be terminated at the load balancer.

How you select ingress routing influences load balancing choices. You’re going to want both.

Ingress Routing - Layer 7
Service Type:LoadBalancer - Layer 4
Ingress Routing is provided natively by NSX-T.  Third party options include Istio or Nginx running as containers in the cluster. Wildcard DNS entries are needed to point at the ingress service in the style of go-routers in PAS. Domain info for ingress is defined in the manifest of the kubernetes deployment. Here is an example.

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: music-ingress
  namespace: music1
spec:
  rules:
  - host: music1.pks.domain.com
    http:
      paths:
      - path: /.*
        backend:
          serviceName: music-service
          servicePort: 8080
```

Service Type:LoadBalancer - Layer 4
When pushing a kubernetes deployment with type set to “LoadBalancer”, NSX-T automatically creates a new VIP for the deployment on the existing load balancer for that namespace. You will need to specify a listening and translation port in the service, along with a name for tagging. You will also specify a protocol to use. Here is an example.

apiVersion: v1
kind: Service
metadata:
  ...
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: web



#### <a id="pks-nsx-t-subnets-ips"></a> Networks, Subnets, and IP Spacing Planning

Allocate a large address block for PKS clusters and PODS block.

Networks defined for PKS is outlined in base ref arch section https://docs-pcf-staging.cfapps.io/pivotalcf/2-6/plan/index.html#pks-network.

In addition , allocate large block following network CIDRs. These can be configured during PKS cluster configuration in OpsManager.
PKS Clusters 172.24.0.0/14
PKS Pods 172.28.0.0/14

When deploying PKS via Ops Manager, you must allow for a block of address space for dynamic networks that PKS will deploy per namespace. The recommended address space is distinctly different  that used with PAS, to give the operator a quick visual queue as to which jobs relate to what service.

When new PKS cluster is  created, PKS dynamically creates a new /24 network is carved out from PKS cluster address space.  When a new application is deployed, a new NSX-T Tier-1 routers are generated and /24 is created from the PKS PODS network.

#### <a id="pks-nsx-t-external-routing"></a> External Routing

Routable external IPs on the Provider side, such as external IPs for NATs or load balancers, are assigned to the T0 router on the front end of the PCF deployment. 

You can assign address space blocks to this job in one of the following ways:

* **Without PKS or With PKS Ingress**: The T0 router needs some routable address space to advertise on the BGP network with its peers. Select a network range with ample address space that can be split into two logical jobs, one job being advertised as a route for traffic and the other job is for aligning T0 DNATs/SNATs, load balancers and other jobs. Unlike NSX-V, you will consume much more address space for SNATs than before.
* **With PKS No Ingress**: Relative to above, this approach has much higher address space consumption for load balancer VIPs, so allow for 4x the address space due to the fact that Kubernetes service types allocate addresses very frequently.

Provider Routable Address Space: /25 (or /23 for PKS No Ingress)

NSX-T handles the routing between a T0 router and any T1 routers associated with it. There are no design parameters needed from Pivotal to accomplish this.


### <a id="pks-nsx-t-ha"></a> High Availability

For information about high availability considerations for PKS on vSphere deployments, see [High Availability](./index.html#pks-ha) in _Platform Architecture and Planning_.


### <a id="pks-nsx-t-storage"></a> Storage

PKS on vSphere supports stateful apps using PersistentVolumes (PVs).

For information about PersistentVolume storage considerations for PKS on vSphere, see [PersistentVolume Storage Options on vSphere](https://docs.pivotal.io/runtimes/pks/1-4/vsphere-persistent-storage.html) in the PKS documentation.


### <a id="pks-nsx-t-security"></a> Security

For information about security considerations for PKS on vSphere deployments, see the [Security](./index.html#pks-security) section in _Platform Architecture and Planning_.


## <a id="pas-pks"></a> PAS and PKS on vSphere with NSX-T

A fully meshed installation of PCF v2.2 or later designed using the recommendations provided in this topic looks as follows:

<%= image_tag('../images/pas-pks-nsxt.png') %>

[View a larger version of this diagram](../images/pas-pks-nsxt.png)

Common components are the NSX T0 router and the associated T1 routers.
This approach ensures that any cross-traffic between PKS and PAS apps stays within the bounds of the T0.
This also provides a one-stop access point to the whole installation, which simplifies deployment automation for multiple, identical installations.

AZs are aligned to vSphere clusters, with resource pools as an optional organizational tool to place multiple foundations into the same capacity.
You can align PKS to any AZ or cluster.
Keeping PKS and PAS in completely separate AZs is not required.

You can use resource pools in vSphere clusters as AZ constructs to stack different installations of PCF. 
As server capacity continues to increase, the efficiency of deploying independent server clusters only for one installation is low. For example, customers are commonly deploying servers with 768-GB RAM and greater.

To allow for max capacity growth, consider using an NSX-T installation per foundation.

If you want to split the PAS and PKS installations into separate network trees, behind separate T0 routers, ensure that approach meets your needs by reviewing VMware’s recommendations for T0 to T0 routing.
For more information,
see [Reference Design Guide for PAS and PKS with VMware NSX-T Data Center](https://communities.vmware.com/docs/DOC-38609) in the VMware documentation.
