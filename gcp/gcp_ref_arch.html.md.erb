---
title: Reference Architecture for Pivotal Cloud Foundry on GCP
owner: Customer0
---

<strong><%= modified_date %></strong>

This guide presents the reference architecture for Pivotal Cloud Foundry (PCF) on Google Cloud Platform (GCP).

## <a id="arch_diagram"></a> Base Reference Architecture 

The following diagram illustrates a base reference architecture deployment of PCF on GCP.

<%= image_tag('gcp-overview-arch.png') %>

To view a larger version of this diagram, click [here](https://raw.githubusercontent.com/c0-ops/landingpage/master/static/gcp/images/PCF-GCP-RefArch-Overview/overview-arch.png).

### <a id="base_components"></a> Base Reference Architecture Components

The following table lists the components that are part of a reference architecture deployment with three availability zones.

|**Component**| **Reference Architecture Notes**|
|--------------------|------------|
|**Domains & DNS** | CF Domain Zones and routes in use by the reference architecture include: *.apps and *.system (required), opsman (required), doppler (required), loggregator (required), ssh (optional) and tcp (optional). Reference architecture uses GCP Cloud DNS as the DNS provider. |
|**Ops Manager**| Deployed on the infrastructure subnet and accessible by FQDN or via an optional Jumpbox. |
|**Bosh Director** | Deployed on the infrastructure subnet. |
|**GoRouter(s)**| Accessed via the HTTP and TCP WebSockets load balancers. Deployed on the ERT subnet, one job per availability zone. | 
|**Diego Brain(s)**| Accessed via the SSH TCP load balancer. Deployed on the ERT subnet, one job per availability zone. |
|**TCP Router(s)** | Optional feature. Deployed on the ERT subnet, one job per availability zone.|
|**CF Blob Storage and Database**| Reference architecture uses GCP Cloud SQL rather than internal MySQL. |
|**Storage buckets** | For buildpacks, droplets, packages and resources. Reference architecture uses Google Cloud Storage. |
|**Services**|Deployed on the PCF managed services subnet. Each service is deployed to each availability zone. |


## <a id="common_network"></a> Common PCF on GCP Network Layouts ##

This section describes the possible network layouts for PCF deployments as covered by the reference architecture of PCF on GCP. All reference architectures described here are production configuration deployments with multiple (3+) AZs.

At a high level, there are currently two possible network layout options as described by the reference architecture:

1. NATs provide public access to PCF internals, or
2. Every PCF VM receives its own public IP address (no NAT).

### <a id="nat_network_diagram"></a> NAT-based Solution

This type of deployment covers the case where you want to expose only a minimal number of public IPs.

The following diagram illustrates the network topology for this solution:

<%= image_tag('gcp-net-topology-base.png') %>

To view a larger version of this diagram, click [here](https://raw.githubusercontent.com/c0-ops/landingpage/master/static/gcp/images/PCF-GCP-RefArch-Overview/net-topology-base.png).

### <a id="no_nat"></a> Public IPs Solution

If you prefer not to use a NAT solution, you can configure PCF on GCP to assign public IP addresses for all components. This type of deployment may be more performant since most of the network traffic between Cloud Foundry components are routed through the front end load balancer and the GoRouter.

### <a id="network_components"></a> Network Objects

The following table lists the network objects expected for each type of reference architecture deployment with three availability zones.

|**Network Object**| **Notes**| **Minimum Number: NAT-based** | **Minimum Number: Public IPs**|
|--------------------|------------|----------------------|---------------------------|
| **External IPs** | For a NAT solution, use 1) global IP for apps and system access 2) Ops Manager (or optional Jumpbox) | 2 | 30+ |
| **NATs** | One NAT per availability zone. | 3 | 0 |
| **Network** | One per deployment. GCP Network objects allow multiple subnets with multiple CIDRs, so a typical deployment of PCF will likely only ever require one GCP Network object | 1 | 1 |
| **Subnets** | Separate subnets for 1) infrastructure (Ops Manager, Ops Manager Director, Jumpbox) 2) ERT 3) services. Using separate subnets allows you to configure different firewall rules due to your needs. | 3 | 3 |
| **Routes** | Routes are typically created by GCP dynamically when subnets are created, but you may need to create additional routes to force outbound communication to dedicated SNAT nodes. These objects are required to deploy PCF without public IP addresses. | 3+ | 3|
| **Firewall Rules** | GCP firewall rules are bound to a Network object and can be created to use IP ranges, subnets, or instance tags to match for source & destination fields in a rule. The preferred method use in the reference architecture deployment is instance tags. | 6+ | 6+ |
| **Load balancers**| Used to handle requests to GoRouters and infrastructure components. GCP uses 2 or more load balancers. The HTTP load balancer and TCP Websockets load balancer are both required. The TCP Router load balancer used for TCP routing feature and the SSH load balancer that allows SSH access to Diego apps are both optional. HTTP load balancer provides SSL termination. | 2+ | 2+ |
| **Jumpbox** | Optional. Provides a way of accessing different network components. For example, you can configure it with your own permissions and then set it up to access to Pivotal Network to download tiles. Using a Jumpbox is particularly useful in IaaSes where Ops Manager does not have a public IP. In these cases, you can SSH into Ops Manager or any other component via the Jumpbox. | (1) | (1)|







