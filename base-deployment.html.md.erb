---
breadcrumb: Pivotal Cloud Foundry Documentation
title: PCF Base Reference Architectures
owner: Customer0
---

This topic describes the base reference architectures for installing Pivotal Cloud Foundry (PCF) on any infrastructure to support the runtime environments Pivotal Application Service (PAS) and Pivotal Container Service (PKS).

For infrastructure-specific reference architectures that build on these two common base architectures, see the following topics:

- [Reference Architecture for PCF on AWS](./aws/aws_ref_arch.html)
- [Reference Architecture for PCF on Azure](./azure/azure_ref_arch.html)
- [Reference Architecture for PCF on GCP](./gcp/gcp_ref_arch.html)
- [Reference Architecture for PCF on OpenStack](./openstack/openstack_ref_arch.html)
- [Reference Architecture for PCF on vSphere](./vsphere/vsphere_ref_arch.html)

## <a id="overview"></a>Overview

A Pivotal Cloud Foundry (PCF) reference architecture describes a proven
approach for deploying PCF on a specific IaaS, such
as AWS, Azure, GCP, OpenStack, or vSphere.
PCF reference architectures meet the following requirements:

  - Secure
  - Publicly-accessible
  - Includes common PCF-managed services such as MySQL, RabbitMQ, and
    Spring Cloud Services
  - Can host at least 100 app instances, or far more

You can use PCF reference architectures to help plan the best configuration for your PCF deployment on your IaaS.

Pivotal has validated the following PCF products on its own deployments
based on these reference architectures:

  - Pivotal Cloud Foundry Ops Manager
  - Pivotal Application Service (PAS)
  - Pivotal Container Services (PKS)

## <a id="pas"></a>PAS Architecture

The following diagram shows a base architecture for PAS, and how its network topology places and replicates PCF and PAS components across subnets and Availability Zones.

<%# Image from PAS_Base canvas of /images/v2/PCF_Reference_Architecture_2019.graffle %>
![PAS Base Deployment Topology](./images/v2/export/PAS_Base.png)

[View a larger version of this diagram](./images/v2/export/PAS_Base.png)

### <a id="pas-components"></a>Internal Components

The following table describes the internal component placements shown in the diagram above:

| **Component**           | **Placement and Access Notes**                                                                                                                                         |
|---------------------- |----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Ops Manager**         | Deployed on one of the three public subnets.  Accessible by FQDN or through an optional jumpbox.                                                                       |
| **BOSH Director**       | Deployed on the infrastructure subnet.                                                                                                                                   |
| **Jumpbox**            | Optional. Deployed on the infrastructure subnet for accessing PAS management components such as Ops Manager and the Cloud Foundry command-line interface (cf CLI).                                            |
| **Gorouters** (HTTP routers in CF)          | Deployed on all three PAS subnets, one per Availability Zone (AZ).<br \>  Accessed through the HTTP, HTTPS, and SSL load balancers.                          |
| **Diego Brain**        | Deployed on all three PAS subnets, one per AZ.<br \>  The Diego Brain component is required, but SSH container access support through the Diego Brain is optional, and enabled through the SSH load balancers.  |
| **TCP Routers**         | Optional. Deployed on all three PAS subnets, one per AZ, to support TCP routing.                                                                                         |
| **Service Tiles**       | Service brokers and shared services instances are deployed to the Services subnet.<br \>  Dedicated on-demand service instances are deployed to an On-demand services subnet.          |
| **Isolation Segments**  | Deployed on an Isolation Segment subnet.  Includes Diego Cells and Gorouters for running and accessing apps hosted within isolation segments.                             |

### <a id="pas-network"></a>Networks

Pivotal recommends defining your networks and load-balancing their incoming requests as follows.

#### <a id="pas-subnets"></a>Required Subnets

PAS requires the following statically-defined networks to host its main component systems.

  - Infrastructure subnet - `/24` segment  
    This subnet contains VMs that require access only for Platform Administrators, for example Ops Manager, the BOSH Director, and an optional jumpbox.
  - PAS subnet - `/24` segment  
    This subnet contains PAS runtime VMs, such as Gorouters, Diego Cells and Cloud Controllers.
  - Services subnet - `/24` segment  
    The Services and On-Demand Services networks support Ops Manager tiles that
    you might add in addition to PAS.
    You can think of them as the the “everything not PAS” networks.
    Some services tiles can call for additional network capacity to grow into on-demand.
    If you use services with this capability, Pivotal recommends that you add an On-Demand Services network for each on-demand service, as described below.
  - On-Demand Services subnet(s) - `/24` segments  
    This is for services that can allocate network capacity on-demand from BOSH for their worker VMs; see the Services subnet above.  
    Pivotal recommends allocating a dedicated subnet to each on-demand service. For example, you can configure the Redis tile as follows:
      - **Network**: enter the existing `Services` network, to host the service broker
      - **Services Network**: deploy a new network `OD-Services1`, to host the Redis worker VMs

    Another on-demand service tile can then also use `Services` for its broker and a new `OD-Services2` network for its workers, and so on.
  - Isolation Segments subnet(s) - `/24` segments  
    You can add one or more Isolation Segment tiles to an PAS installation to compartmentalize hosting and routing resources.
    Each Isolation Segment has its own range of address space that requires a `/24` network.
    Each `/24` network has the capacity for \>50 Isolation Segments hosting \>250 VMs each.

#### <a id="pas-lb"></a>Load Balancing

Any PAS installation needs a suitable load balancer to send incoming HTTP, HTTPS, SSH and SSL traffic to its Gorouters and application containers.
All installations approaching Production level use rely on external load balancing from hardware appliance vendors or other network-layer solutions.

The load balancer can also perform Layer 4 or Layer 7 load balancing functions.
SSL can be terminated at the load balancer or used as a pass-through to the Gorouter.

Common deployments of load balancing in PAS are:

  - HTTP/HTTPS traffic to and from Gorouters
  - TCP traffic to and from TCP routers
  - Traffic from the Diego Brain, when developers access app containers via SSH

To balance load across multiple PAS foundations, use an IaaS- or vendor-specific Global Traffic Manager or Global DNS load balancer.

For additional information, see [Global DNS Load Balancers for Multi-Foundation Environments](./global-dns-lb.html).

### <a id="pas-ha"></a>High Availability

PAS isn’t considered High Availability (HA) until it runs across at least two Availability Zones (AZs).
Pivotal recommends defining three AZs.

On IaaSes with their own HA capabilities, using the IaaS HA in conjunction with a PCF HA topology provides the best of both worlds.
Multiple AZs give PCF redundancy, so that losing an AZ isn’t catastrophic.
The BOSH Resurrector can then replace lost VMs as needed to repair a foundation.

To back up and restore a foundation externally, use [BOSH Backup and Restore](https://docs.cloudfoundry.org/bbr/) (BBR).

### <a id="pas-storage"></a>Storage

PAS requires persistent storage disks for its components to store their state. You allocate these in the PAS tile's **Resource Config** pane.
For details on storage configuration and capacity planning, refer to the corresponding chapter for the specific IaaS.

The platform also requires you to configure external or internal file storage, also called the blobstore. For details, see [Configuring File Storage for PAS](../customizing/pas-file-storage.html).

### <a id="pas-security"></a>Security

See the topics below for how PAS implements security:

* [PCF Infrastructure Security
](../security/pcf-infrastructure/index.html)

* [Security Concepts](../security/concepts/)

* [Certificates and TLS in PCF](../security/pcf-infrastructure/certificates-index.html)

* [Network Communication Paths in PCF](../security/networking/#net_commpaths) - includes firewall ports

### <a id="pas-domains"></a>Domain Names

PAS requires following domain names to be registered:

- System domain, for PAS and other tiles: `sys.domain.name`
- App domain, for your applications: `app.domain.name`

You must also define the following wildcard domain names and include them when creating certificates that access PAS and its hosted apps:

  - \*.SYSTEM-DOMAIN
  - \*.APPS-DOMAIN
  - \*.login.SYSTEM-DOMAIN
  - \*.uaa.SYSTEM-DOMAIN

### <a id="pas-scaling"></a>Component Scaling

See [Scaling PAS](../opsguide/scaling-ert-components.html) for recommendations on how to scale PAS for different deployment scenarios.

## <a id="pks-topology"></a>PKS Architecture

The following diagram shows a base architecture for PKS, and how its network topology places and replicates PCF and PKS components across subnets and Availability Zones.

<%# Image from PKS_Base canvas of /images/v2/PCF_Reference_Architecture_2019.graffle %>
![PAS Base Deployment Topology](./images/v2/export/PKS_Base.png)

[View a larger version of this diagram](./images/v2/export/PKS_Base.png)

### <a id="pks-components"></a>Internal Components

The following table describes the internal component placements shown in the diagram above:

| **Component**           | **Placement and Access Notes**                                                                                                                                         |
|---------------------- |----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Ops Manager**         | Deployed on one of the subnets. Accessible by fully-qualified domain name (FQDN) or through an optional jumpbox.   |
| **BOSH Director**         | Deployed on the infrastructure subnet.   |
| **Jumpbox**         | Optional. Deployed on the infrastructure subnet for accessing PKS management components such as Ops Manager and the PKS + Kubectl command-line interface.   |
| **PKS API**         | Deployed as a service broker VM on the PKS Services subnet.  Handles PKS API and service adapter requests, and manages PKS clusters.  For more information, see [Pivotal Container Service (PKS)](https://docs.pivotal.io/runtimes/pks/index.html#components) |
| **Harbor Tile**         | Optional container images registry, typically deployed to the Services subnet.   |
| **PKS Cluster**         | Deployed to a dynamically-created, dedicated PKS cluster subnet. Each cluster consists of worker nodes that run the workloads (applications) and one or more master nodes.   |

### <a id="pks-network"></a>Networks

#### <a id="pks-subnets"></a>Subnets Requirements

PKS requires two defined networks to host the main elements it’s
composed of.

  - Infrastructure subnet - `/24`  
    Contains VMs that require access only for Platform Administrators, for example Ops Manager, BOSH Director and an optional jumpbox.
  - PKS Services subnet - `/24`  
    Hosts PKS API VM and other optional service tiles such as
    Harbor.
  - PKS Clusters subnets - each one a `/24` from a pool of pre-allocated IPs  
    Hosts PKS clusters.

#### <a id="pks-lb"></a>Load Balancing

Load balancers can be used to manage traffic across master nodes of a PKS cluster or for deployed workloads. For more information on how to configure load balancers for PKS, refer to the corresponding documentation for the specific IaaS.

### <a id="pks-ha"></a>High Availability

PKS has no inherent HA capabilities to design for. Make the best efforts
to have HA design at the IaaS, storage, power and access layers to
support PKS.

### <a id="pks-storage"></a>Storage

PKS requires shared storage across all Availability Zones for the
deployed workloads to appropriately allocate their required storage.

**Capacity**

TBD: any generic notes on storage and capacity planning? Or leave it to
IaaS specific sections only?

### <a id="pks-security"></a>Security

See the topics below for how PKS implements security:

* [Enterprise PKS Security](https://docs.pivotal.io/runtimes/pks/security.html)

* [Firewall Ports](https://docs.pivotal.io/runtimes/pks/ports-protocols-nsx-t.html)

### <a id="pks-domains"></a>Domain Names

PKS requires the following domain names to be registered when creating a
wildcard certificate and PKS tile configurations: `*.pks.domain.name`

The wildcard certificate covers both the PKS API domain e.g. `api.pks.domain.name` and the PKS Clusters
domains e.g. `mycluster.pks.domain.name`.

### <a id="pks-management"></a>Cluster Management

See [Managing Clusters](https://docs.pivotal.io/runtimes/pks/managing-clusters.html) for recommendations on managing Enterprise PKS clusters.
